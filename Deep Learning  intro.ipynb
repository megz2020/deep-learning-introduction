{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center><font face=\"verdana\" color=\"black\">Deep Learning </font></center></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start let's ask ourselves<font size=\"5\" color=\"blue\">why Deep Learning?</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can I know if Deep Learning works better for a specific problem than SVM for example? <br>\n",
    "i think it depends on data you have and  What performance score is desired?<br>\n",
    "As a rule of thumb, I’d say that SVMs are great for relatively small data sets with fewer outliers.on the other hand, deep learning really shines when it comes to complex problems such as image classification, natural language processing, and speech recognition. Another advantage is that you have to worry less about the feature engineering part.<br>\n",
    "so in my opinion: \n",
    "* we start with the simplest model\n",
    "* If you don’t meet your expected goal, try more complex models (if possible)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dl5-768x557.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what deep learning is, we first need to understand the relationship deep learning has with machine learning, neural networks, and artificial intelligence.\n",
    "\n",
    "<img src=\"dl3.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning is a specific subset of Machine Learning which is using a specific algorithm called a Neural Network<br>\n",
    "**so What is a neural network?**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks inspierd by brain analogies when describing them also small unit of  Neural Networks  look like neurones in brain. Without delving into brain analogies, I find it easier to simply describe Neural Networks as a mathematical function that maps a given input to a desired output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so let's start with simple example: <br>\n",
    "we need to predict house prices based on area of house <br>\n",
    "a simple solution for this problem would be to get the average price per sq ft and multiply it in total area of house <br>\n",
    "so now we have<br>\n",
    "* input x we can called it `feature`\n",
    "* the average price per sq ft we can called it `weight` \n",
    "* output this is our prediction value `y`\n",
    "if we put all together we have this function:<br>\n",
    "## `y = x*w`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"simple_NN_1.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is simple block of neural network not efficient but this is first one we will make it efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we see weight has great effect on our feature it tell us how much this feature important in our prediction. Here we started with an average, later we’ll look at better algorithms that can scale as we get more inputs and more complicated models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function will give us line(since the output can be of continuous values, the technical name for what we have would be a “regression model”)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"1_BFzp8uSMk88mDLibU465VA.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we want to move our line up and down to make your prediction better what we do? <br>\n",
    "we will add another factor called bias <br>\n",
    "it just like tell our function hey be ready till i say start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"model1.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's convert our examble from regression to classification:<br>\n",
    "we want our model predict this pict is cat or not. what we need? <br>\n",
    "* we will add function convert our output to (0 or 1) we called this Activation function<br>\n",
    "\n",
    "and we called our block perceptron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perceptron is asingle layer neaural network it consist of:\n",
    "* input values or input layer\n",
    "* weights and bias\n",
    "* net sum\n",
    "* acctivation function or ouput layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"1_n6sJ4yZQzwKL9wnF5wnVNg.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how does it work?<br>\n",
    "like before but we don't take average\n",
    "* Start with random weights\n",
    "* All the input x are multiplied with their weights \n",
    "* Add all the multiplied values and calll it weighted sum\n",
    "* Apply the weighted sum to the correct activation\n",
    "   *  if 0 > x it give us 0\n",
    "   * if x > 0 it give us 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ERROR FUNCTION**<br>\n",
    "we need somthing tell us how much we far from the true point to change our weights and get right one.<br>\n",
    "Imagine yourself on the top of mountain you cannot see the bottom and you want to down. what will you do?<br>\n",
    "* We will look around us and considered all the posible direction in which we can walk\n",
    "* Then we pick adirection that make us close to the bottom \n",
    "* After we take a step we start process again and repeat what we did before till we reach the bottom \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so our function should be:\n",
    "- should be continous\n",
    "   - if it discrete we cann't take any step as all the posible direction will have the same erorr\n",
    "- should be differentiable \n",
    "   - as we we change our weight and see how this effect on our output which mean rate of change of a function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to convert from discrete to continuous we apply outputs to sigmoid function which give us numbers fom(0 to 1) represent the probability of this point being positive(class 1) or negative(class 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"SigmoidFunction_701.gif\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we have multi-class we use another function called softmax function .\n",
    "<img src=\"1_hwdjtUG2pv8EhuxcR4mWmA.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how we pick right model based on their probabilties?<br>\n",
    "we assume every class is independent event so \n",
    "- p(total) = p1 * p2 .....*pn\n",
    "- but this will give us very small value like (0.00000000122)\n",
    "- so we will take minus log for every probability as \n",
    "  - log(AB) = log(A) + log(B) \n",
    "  - we add minus to avoid negative number as log numbe < 1 give us negative value.\n",
    "- we called this `CROSS ENTROPY`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CROSS ENTROPY**:\n",
    "- CROSS ENTROPY is the sum of the negative logarithms of the probability.\n",
    "- The bad model ==> High cross entropy\n",
    "- The good model ==> low cross entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so let's calculate the error by take differance \n",
    "- The total propapilty equal to one\n",
    "- take the avarge of cross entropy\n",
    "\n",
    "error = - 1/m (sum((1 - y) log(1 - y_predict) + y log(y_predict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we will take differentiation for every point<br>\n",
    "i will write just finall formula\n",
    "- aE/aw = -(y - ypredict)xi\n",
    "- aE/ab = - (y -ypreict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we will ubdate our weights and bias to take our step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we called what we did `Gradient Descent`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**psedo code for Gradient Descent**<br>\n",
    "- start with random weights\n",
    "- for every point (x1,....,xn)\n",
    "  - for i = 1 to n:\n",
    "     - update \n",
    "$$ w_i \\longrightarrow w_i + \\alpha (y - \\hat{y}) x_i$$\n",
    "$$ b \\longrightarrow b + \\alpha (y - \\hat{y})$$\n",
    "- repeat until to get smallest error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note we add alpha(learning rate) to control in our step we don't want dramatic changes\n",
    "- if alpha too small we will take too long time\n",
    "- if it too high we can skip the point we want to reach like we reach the bottom and skip it and move to another mountain because our step is too big\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we ready to build deep neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It multi-layer of perceptron combine linear models to get complex one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Capture.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How we train neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The output y of a simple 2-layer Neural Network is:**<br>\n",
    "<img src=\"eq.PNG\" />\n",
    "\n",
    "#### You might notice that in the equation above, the weights W and the biases b are the only variables that affects the output y.\n",
    "#### Naturally, the right values for the weights and biases determines the strength of the predictions. The process of fine-tuning the weights and biases from the input data is known as training the Neural Network.\n",
    "#### Each iteration of the training process consists of the following steps:\n",
    "- Calculating the predicted output ŷ, known as feedforward\n",
    "- Updating the weights and biases, known as backpropagation\n",
    "<img src=\"train.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to make neural networks and do feed forward\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y):\n",
    "        self.input      = x\n",
    "        self.weights1   = np.random.rand(self.input.shape[1],4) \n",
    "        self.weights2   = np.random.rand(4,1)                 \n",
    "        self.y          = y\n",
    "        self.output     = np.zeros(self.y.shape)\n",
    "\n",
    "    def feedforward(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "        self.output = sigmoid(np.dot(self.layer1, self.weights2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagation**<br>\n",
    "Now that we’ve measured the error of our prediction (loss), we need to find a way to propagate the error back, and to update our weights and biases we called this `Backpropagation`<br>\n",
    "it consist of: <br>\n",
    "- Doing feed dorward operation\n",
    "- comparing the output of the model with the desierd output\n",
    "- calculating the error\n",
    "- running the feed forward operation backword\n",
    "- use this to update model and get better result\n",
    "- repeat until get good result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code with back propagation\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y):\n",
    "        self.input      = x\n",
    "        self.weights1   = np.random.rand(self.input.shape[1],4) \n",
    "        self.weights2   = np.random.rand(4,1)                 \n",
    "        self.y          = y\n",
    "        self.output     = np.zeros(self.y.shape)\n",
    "\n",
    "    def feedforward(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "        self.output = sigmoid(np.dot(self.layer1, self.weights2))\n",
    "\n",
    "    def backprop(self):\n",
    "        # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1\n",
    "        d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))\n",
    "        d_weights1 = np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * \n",
    "                                                   sigmoid_derivative(self.output), self.weights2.T) *\n",
    "                                            sigmoid_derivative(self.layer1)))\n",
    "         # update the weights with the derivative (slope) of the loss function\n",
    "        self.weights1 += d_weights1\n",
    "        self.weights2 += d_weights2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to optmize the training model ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note number of steps or how many time we update weights and bias is called (Epoch)\n",
    "- if we start with (epoch = 1) , we will get simple model cannot predict so well which called `under fitting`\n",
    "- if we train (epoch = 20) we can get pretty good result\n",
    "- if we train(epoch = 1000) we can get good result on train data set but bad result in test data set which called `over fitting`\n",
    "so we run our grade decent untill the testing errors stops decreasing and start increase.<br>\n",
    "and we will try some methods to prevent over fitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularization***<br>\n",
    "Regularization modifies the Error function by adding additional terms that penalize large weights. In other words, we change the Error function so that it becomes<br>\n",
    "- Error+λ(|w|+ ....+|wn|), where λ is the regularization strength (a hyper-parameter for the learning algorithm).this is called `L1`\n",
    "- Error+λ (w**2 + ....+wn**2) this is called `L2`\n",
    "- The value we choose for λ determines how much we want to protect against overfitting. \n",
    "  - lambda is larg ==> we penalized them alot\n",
    "  - lambda is small ==> we not penalized them alot<br>\n",
    "  \n",
    "we can try l1 or l2 to prevent over fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropout**<br>\n",
    "This is another way to prevent overfitting<br>\n",
    "when we train our model somtimes <br>\n",
    "- one part has larg weight and ends up dominating\n",
    "- another part dosenot\n",
    "\n",
    "so we do somthing called drop out which mean turn off dominate part and train our neural network<br>\n",
    "in real what we do is give the algorithm paramete which is the probailty that each node get dropped<br>\n",
    "we repeat this so every node will treat in equality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### local minimum problem<br>\n",
    "som times we have local minimum which mean alot of point look like bottom so what we do ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Restart**<br>\n",
    "one way to solve local minimum <br>\n",
    "- we start from differant places and take gradient descend from all of them\n",
    "\n",
    "This increase the probabilty that we get the gloable minimum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sigmoid problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we calculate the derivative at point away at the right or away at left we will get small value this not good for gradient descent this will take long time<br>\n",
    "so except ouput layer we will change sigmoid and add another activation function like\n",
    "- HyperBolic Tangent function\n",
    "- RECTIFIED LINEAR UNIT (RELU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
